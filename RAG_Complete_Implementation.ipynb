{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/js08080/AIAccelarator/blob/main/RAG_Complete_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e9123a",
      "metadata": {
        "id": "21e9123a"
      },
      "source": [
        "# RAG Implementation with LlamaIndex and LanceDB\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval Augmented Generation) implementation using LlamaIndex and LanceDB. We'll explore three different approaches:\n",
        "\n",
        "1. **Vector Search Only** - Fast retrieval without LLM generation\n",
        "2. **HuggingFace API Integration** - Cloud-based LLM with authentication\n",
        "3. **Local LLM with Ollama** - Complete local solution\n",
        "\n",
        "## Overview\n",
        "\n",
        "The notebook covers:\n",
        "- Data loading and preparation from HuggingFace datasets\n",
        "- Vector store setup with LanceDB\n",
        "- Embedding generation with HuggingFace models\n",
        "- Three different query approaches with increasing complexity\n",
        "- Utility functions for table exploration and optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bf9504",
      "metadata": {
        "id": "a7bf9504"
      },
      "source": [
        "## 1. Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc3a44d",
      "metadata": {
        "id": "6dc3a44d"
      },
      "outputs": [],
      "source": [
        "# # Install all required packages\n",
        "# !pip install llama-index llama-index-vector-stores-lancedb llama-index-embeddings-huggingface llama-index-llms-huggingface-api lancedb datasets -q\n",
        "\n",
        "# # Additional packages for local LLM and utilities\n",
        "# !pip install llama-index-llms-ollama requests -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae153d74",
      "metadata": {
        "id": "ae153d74"
      },
      "source": [
        "## 2. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51b6153",
      "metadata": {
        "id": "b51b6153",
        "outputId": "d3c70ed4-d0bf-416e-a583-cc81850dd6ce"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import lancedb\n",
        "import subprocess\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Embedding and vector store\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "\n",
        "# LLM integrations\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# Async support for notebooks\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7dccac",
      "metadata": {
        "id": "fe7dccac"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6297a29",
      "metadata": {
        "id": "b6297a29"
      },
      "source": [
        "## 3. Data Preparation and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df8ae93",
      "metadata": {
        "id": "1df8ae93",
        "outputId": "680e18cb-ad71-4308-a622-6575a5be953b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 100 personas from dataset...\n",
            "Prepared 100 documents\n"
          ]
        }
      ],
      "source": [
        "def prepare_data(num_samples=100):\n",
        "    \"\"\"\n",
        "    Load dataset and create document files\n",
        "    \"\"\"\n",
        "    print(f\"Loading {num_samples} personas from dataset...\")\n",
        "\n",
        "    # Load the personas dataset\n",
        "    dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
        "\n",
        "    # Create data directory\n",
        "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save personas as text files and create Document objects\n",
        "    documents = []\n",
        "    for i, persona in enumerate(dataset.select(range(min(num_samples, len(dataset))))):\n",
        "        # Create Document objects for LlamaIndex\n",
        "        doc = Document(\n",
        "            text=persona[\"persona\"],\n",
        "            metadata={\n",
        "                \"persona_id\": i,\n",
        "                \"source\": \"finepersonas-dataset\"\n",
        "            }\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "        # Optionally save to files as well\n",
        "        with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(persona[\"persona\"])\n",
        "\n",
        "    print(f\"Prepared {len(documents)} documents\")\n",
        "    return documents\n",
        "\n",
        "# Load the data\n",
        "documents = prepare_data(num_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1def1b98",
      "metadata": {
        "id": "1def1b98"
      },
      "source": [
        "## 4. LanceDB Vector Store Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6741ba12",
      "metadata": {
        "id": "6741ba12",
        "outputId": "66ea1df6-cfa1-425f-c6ec-e653a34c9e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up LanceDB connection...\n",
            "Connected to LanceDB, table: personas_rag\n"
          ]
        }
      ],
      "source": [
        "def setup_lancedb_store(table_name=\"personas_rag\"):\n",
        "    \"\"\"\n",
        "    Initialize LanceDB and create/connect to a table\n",
        "    \"\"\"\n",
        "    print(\"Setting up LanceDB connection...\")\n",
        "\n",
        "    # Create or connect to LanceDB\n",
        "    db = lancedb.connect(\"./lancedb_data\")\n",
        "\n",
        "    # LlamaIndex will handle table creation with proper schema\n",
        "    print(f\"Connected to LanceDB, table: {table_name}\")\n",
        "\n",
        "    return db, table_name\n",
        "\n",
        "# Setup database connection\n",
        "db, table_name = setup_lancedb_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794d2ce0",
      "metadata": {
        "id": "794d2ce0"
      },
      "source": [
        "## 5. Vector Embeddings and Ingestion Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec8a431",
      "metadata": {
        "id": "7ec8a431",
        "outputId": "dbbc730c-89ee-4bb0-8ec7-d9b8cbbf2945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating embedding model and ingestion pipeline...\n",
            "Processing documents and creating embeddings...\n",
            "Successfully processed 100 text chunks\n"
          ]
        }
      ],
      "source": [
        "async def create_and_populate_index(documents, db, table_name):\n",
        "    \"\"\"\n",
        "    Create ingestion pipeline and populate LanceDB with embeddings\n",
        "    \"\"\"\n",
        "    print(\"Creating embedding model and ingestion pipeline...\")\n",
        "\n",
        "    # Initialize embedding model\n",
        "    embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "    )\n",
        "\n",
        "    # Create LanceDB vector store\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=\"./lancedb_data\",\n",
        "        table_name=table_name,\n",
        "        mode=\"overwrite\"  # overwrite existing table\n",
        "    )\n",
        "\n",
        "    # Create ingestion pipeline\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
        "            embed_model,\n",
        "        ],\n",
        "        vector_store=vector_store,\n",
        "    )\n",
        "\n",
        "    print(\"Processing documents and creating embeddings...\")\n",
        "    # Run the pipeline to process documents and store in LanceDB\n",
        "    nodes = await pipeline.arun(documents=documents)\n",
        "    print(f\"Successfully processed {len(nodes)} text chunks\")\n",
        "\n",
        "    return vector_store, embed_model\n",
        "\n",
        "# Create embeddings and populate vector store\n",
        "vector_store, embed_model = await create_and_populate_index(documents, db, table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46affc4e",
      "metadata": {
        "id": "46affc4e",
        "outputId": "61d053d6-f75c-44cc-b225-0211461edd77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'stores_text': True,\n",
              " 'is_embedding_query': True,\n",
              " 'flat_metadata': True,\n",
              " 'uri': './lancedb_data',\n",
              " 'vector_column_name': 'vector',\n",
              " 'nprobes': 20,\n",
              " 'refine_factor': None,\n",
              " 'text_key': 'text',\n",
              " 'doc_id_key': 'doc_id',\n",
              " 'api_key': None,\n",
              " 'region': None,\n",
              " 'mode': 'overwrite',\n",
              " 'query_type': 'vector',\n",
              " 'overfetch_factor': 1,\n",
              " 'class_name': 'base_component'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cd7550",
      "metadata": {
        "id": "b6cd7550"
      },
      "source": [
        "## 6. Option 1: Vector Search Only (No LLM)\n",
        "\n",
        "This approach provides fast document retrieval without LLM generation. Perfect for finding relevant content quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b00c89e",
      "metadata": {
        "id": "1b00c89e",
        "outputId": "6b6fd68d-8b67-4634-ee4c-00ad4d7a1a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Vector Search (No LLM needed)\n",
            "==================================================\n",
            "\n",
            "Query: technology and artificial intelligence expert\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Result 2 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Result 3 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Query: teacher educator professor\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Result 2 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Result 3 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Query: environment climate sustainability\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Result 2 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Result 3 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Query: art culture heritage creative\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
            "\n",
            "Result 2 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
            "\n",
            "Result 3 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n"
          ]
        }
      ],
      "source": [
        "def perform_vector_search(db, table_name, query_text, embed_model, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform direct vector search on LanceDB\n",
        "    \"\"\"\n",
        "    # Get query embedding\n",
        "    query_embedding = embed_model.get_text_embedding(query_text)\n",
        "\n",
        "    # Open table and perform search\n",
        "    table = db.open_table(table_name)\n",
        "    results = table.search(query_embedding).limit(top_k).to_pandas()\n",
        "\n",
        "    return results\n",
        "\n",
        "def test_vector_search():\n",
        "    \"\"\"\n",
        "    Test vector search functionality with sample queries\n",
        "    \"\"\"\n",
        "    print(\"Testing Vector Search (No LLM needed)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test queries\n",
        "    queries = [\n",
        "        \"technology and artificial intelligence expert\",\n",
        "        \"teacher educator professor\",\n",
        "        \"environment climate sustainability\",\n",
        "        \"art culture heritage creative\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Perform search\n",
        "        results = perform_vector_search(db, table_name, query, embed_model, top_k=3)\n",
        "\n",
        "        for idx, row in results.iterrows():\n",
        "            score = row.get('_distance', 'N/A')\n",
        "            text = row.get('text', 'N/A')\n",
        "\n",
        "            # Format score\n",
        "            if isinstance(score, (int, float)):\n",
        "                score_str = f\"{score:.3f}\"\n",
        "            else:\n",
        "                score_str = str(score)\n",
        "\n",
        "            print(f\"\\nResult {idx + 1} (Score: {score_str}):\")\n",
        "            print(f\"{text[:200]}...\")\n",
        "\n",
        "# Run vector search test\n",
        "test_vector_search()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad7668d",
      "metadata": {
        "id": "dad7668d"
      },
      "source": [
        "## 7. Option 2: RAG with HuggingFace API\n",
        "\n",
        "This approach uses HuggingFace's cloud API for LLM generation. Requires API token authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b797dda7",
      "metadata": {
        "id": "b797dda7",
        "outputId": "8e007a59-62f7-4b29-f569-1fe8ed744373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing RAG with HuggingFace API\n",
            "========================================\n",
            "\n",
            "Query: Find personas interested in technology and AI\n",
            "------------------------------\n",
            "Response: <think>\n",
            "Okay, let's see. The user is asking for personas interested in technology and AI. The context provided is about an aerospace engineer or astrobiologist from the finepersonas-dataset. So, I need to figure out if this persona is interested in technology and AI.\n",
            "\n",
            "First, aerospace engineers work on designing and developing aircraft, spacecraft, and missiles. They use a lot of advanced technologies, including AI for simulations, data analysis, and autonomous systems. Astrobiologists study the origin, evolution, distribution, and future of life in the universe. They might use AI for analyzing data from space missions, like searching for signs of life on other planets.\n",
            "\n",
            "So, both roles involve working with technology and AI. The aerospace engineer would definitely be interested in AI for their projects, and the astrobiologist would use AI in their research. Therefore, the persona from the given context is interested in technology and AI. The answer should be the persona_id 89 from the finepersonas-dataset.\n",
            "</think>\n",
            "\n",
            "The persona_id 89 from the finepersonas-dataset is interested in technology and AI.\n",
            "\n",
            "Query: Who are the educators or teachers in the dataset?\n",
            "------------------------------\n",
            "Response: <think>\n",
            "Okay, let's see. The user is asking who the educators or teachers are in the dataset based on the provided context. The context mentions a persona with persona_id 22 from the finepersonas-dataset. The persona is described as an English language arts teacher with a focus on upper elementary education.\n",
            "\n",
            "So, the key points here are that the dataset includes personas, and one of them is an English language arts teacher for upper elementary students. The question is about identifying the educators or teachers in the dataset. Since the context specifically names this persona as an English language arts teacher, that's the main example given. \n",
            "\n",
            "I need to make sure I'm not adding any extra information beyond what's in the context. The user wants the answer based solely on the provided context, not any prior knowledge. The answer should be straightforward, just stating the type of teacher mentioned. There's no mention of other personas or other types of teachers in the context, so the answer is limited to the one described here. \n",
            "\n",
            "Therefore, the correct answer is that the dataset includes an English language arts teacher for upper elementary education. That's the only educator or teacher explicitly mentioned in the context.\n",
            "</think>\n",
            "\n",
            "The dataset includes an English language arts teacher for upper elementary education.\n",
            "\n",
            "Query: Describe personas working with environmental topics\n",
            "------------------------------\n",
            "Response: <think>\n",
            "Okay, let's see. The user is asking to describe personas working with environmental topics. The context provided mentions an environmental scientist focused on climate change and pollution, and a sustainability advocate pushing for global action on reducing greenhouse gas emissions. \n",
            "\n",
            "First, I need to make sure I understand the key points here. The two personas are distinct but related. The environmental scientist is probably more technical, dealing with data, research, and maybe policy. The sustainability advocate is more about advocacy, public engagement, and influencing policy through awareness and activism.\n",
            "\n",
            "I should avoid referencing the persona_id or the source directly. The answer should be general but based on the given info. So, I need to combine elements of both roles. The environmental scientist would be involved in research, data analysis, and developing strategies to address climate change and pollution. The sustainability advocate would focus on education, public campaigns, and collaborating with governments and organizations to implement policies.\n",
            "\n",
            "I should mention their roles in different areas: research, policy, advocacy, and public engagement. Also, highlight their common goal of addressing environmental issues through various methods. Need to make sure the answer is concise and doesn't include any prior knowledge beyond the context given. Let me structure that into a coherent answer.\n",
            "</think>\n",
            "\n",
            "Environmental professionals working on environmental topics\n"
          ]
        }
      ],
      "source": [
        "# Set your HuggingFace API token here\n",
        "# Get your free token from: https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"  # Replace with your actual token\n",
        "\n",
        "def create_query_engine(vector_store, embed_model, llm=None):\n",
        "    \"\"\"\n",
        "    Create a query engine from the vector store\n",
        "    \"\"\"\n",
        "    # Create index from vector store\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    # Setup LLM if provided\n",
        "    query_engine_kwargs = {}\n",
        "    if llm:\n",
        "        query_engine_kwargs['llm'] = llm\n",
        "\n",
        "    # Create query engine\n",
        "    query_engine = index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        **query_engine_kwargs\n",
        "    )\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "def query_rag(query_engine, question):\n",
        "    \"\"\"\n",
        "    Query the RAG system and return response\n",
        "    \"\"\"\n",
        "    response = query_engine.query(question)\n",
        "    return response\n",
        "\n",
        "async def test_huggingface_rag():\n",
        "    \"\"\"\n",
        "    Test RAG with HuggingFace API\n",
        "    \"\"\"\n",
        "    print(\"Testing RAG with HuggingFace API\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Initialize HuggingFace LLM with authentication\n",
        "        llm = HuggingFaceInferenceAPI(\n",
        "            model_name=  \"HuggingFaceH4/zephyr-7b-beta\", #\"HuggingFaceTB/SmolLM3-3B\",\n",
        "            token=os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "        )\n",
        "\n",
        "        # Create query engine\n",
        "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
        "\n",
        "        # Test queries\n",
        "        queries = [\n",
        "            \"Find personas interested in technology and AI\",\n",
        "            \"Who are the educators or teachers in the dataset?\",\n",
        "            \"Describe personas working with environmental topics\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            try:\n",
        "                response = query_rag(query_engine, query)\n",
        "                print(f\"Response: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Setup error: {e}\")\n",
        "        print(\"Make sure to set your HuggingFace API token above\")\n",
        "\n",
        "# Uncomment the line below after setting your API token\n",
        "await test_huggingface_rag()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c437ab16",
      "metadata": {
        "id": "c437ab16"
      },
      "source": [
        "## 8. Option 3: RAG with Local LLM (Ollama)\n",
        "\n",
        "This approach uses a completely local LLM setup. No internet required after initial setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3744e4e",
      "metadata": {
        "id": "b3744e4e"
      },
      "outputs": [],
      "source": [
        "def check_ollama_installed():\n",
        "    \"\"\"Check if Ollama is installed\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"--version\"],\n",
        "                              capture_output=True, text=True, shell=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Ollama is installed: {result.stdout.strip()}\")\n",
        "            return True\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    print(\"Ollama is not installed or not in PATH\")\n",
        "    return False\n",
        "\n",
        "def download_ollama():\n",
        "    \"\"\"Download Ollama installer for Windows\"\"\"\n",
        "    print(\"Downloading Ollama for Windows...\")\n",
        "\n",
        "    url = \"https://ollama.com/download/OllamaSetup.exe\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    installer_path = Path(\"OllamaSetup.exe\")\n",
        "    with open(installer_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(\"Ollama downloaded successfully!\")\n",
        "    print(\"Please run the installer manually and then continue.\")\n",
        "    print(f\"Installer location: {installer_path.absolute()}\")\n",
        "\n",
        "    return installer_path\n",
        "\n",
        "def start_ollama_service():\n",
        "    \"\"\"Start Ollama service\"\"\"\n",
        "    try:\n",
        "        print(\"Starting Ollama service...\")\n",
        "        subprocess.Popen([\"ollama\", \"serve\"], shell=True)\n",
        "        time.sleep(3)\n",
        "        print(\"Ollama service started!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start Ollama: {e}\")\n",
        "        return False\n",
        "\n",
        "def pull_ollama_model(model_name=\"llama3.2:1b\"):\n",
        "    \"\"\"Pull a lightweight model for local inference\"\"\"\n",
        "    try:\n",
        "        print(f\"Pulling model: {model_name}\")\n",
        "        result = subprocess.run([\"ollama\", \"pull\", model_name],\n",
        "                              capture_output=True, text=True, shell=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Model {model_name} pulled successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Failed to pull model: {result.stderr}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error pulling model: {e}\")\n",
        "        return False\n",
        "\n",
        "def setup_ollama():\n",
        "    \"\"\"Complete Ollama setup\"\"\"\n",
        "    if not check_ollama_installed():\n",
        "        print(\"Ollama needs to be installed.\")\n",
        "        download_ollama()\n",
        "        return False\n",
        "\n",
        "    if not start_ollama_service():\n",
        "        return False\n",
        "\n",
        "    if not pull_ollama_model(\"llama3.2:1b\"):\n",
        "        return False\n",
        "\n",
        "    print(\"Ollama setup complete!\")\n",
        "    return True\n",
        "\n",
        "# Check Ollama installation\n",
        "check_ollama_installed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2011bd",
      "metadata": {
        "id": "2d2011bd"
      },
      "outputs": [],
      "source": [
        "async def test_local_llm_rag():\n",
        "    \"\"\"\n",
        "    Test RAG with local Ollama LLM\n",
        "    \"\"\"\n",
        "    print(\"Testing RAG with Local LLM (Ollama)\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Initialize local Ollama LLM\n",
        "        llm = Ollama(\n",
        "            model=\"llama3.2:1b\",\n",
        "            base_url=\"http://localhost:11434\",\n",
        "            request_timeout=60.0\n",
        "        )\n",
        "\n",
        "        # Create query engine\n",
        "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
        "\n",
        "        # Test queries\n",
        "        queries = [\n",
        "            \"Find personas interested in technology and AI\",\n",
        "            \"Who are the educators or teachers in the dataset?\",\n",
        "            \"Describe personas working with environmental topics\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            try:\n",
        "                response = query_rag(query_engine, query)\n",
        "                print(f\"Response: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                print(\"Make sure Ollama is running with: ollama serve\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Setup error: {e}\")\n",
        "        print(\"Make sure Ollama is installed and running\")\n",
        "\n",
        "# Uncomment after Ollama setup is complete\n",
        "# await test_local_llm_rag()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c42b76",
      "metadata": {
        "id": "c1c42b76"
      },
      "source": [
        "## 9. Utility Functions and Advanced Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a81e5b1",
      "metadata": {
        "id": "7a81e5b1",
        "outputId": "2dbeda52-8abb-4343-df80-6423f016b160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage Examples:\n",
            "==============================\n",
            "\n",
            "1. Vector Search Only:\n",
            "   test_vector_search()\n",
            "\n",
            "2. HuggingFace API RAG:\n",
            "   # Set API token first\n",
            "   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\n",
            "   await test_huggingface_rag()\n",
            "\n",
            "3. Local LLM RAG:\n",
            "   # Install and setup Ollama first\n",
            "   setup_ollama()\n",
            "   await test_local_llm_rag()\n",
            "\n",
            "4. Explore Database:\n",
            "   explore_lancedb_table(db, table_name)\n"
          ]
        }
      ],
      "source": [
        "def explore_lancedb_table(db, table_name):\n",
        "    \"\"\"\n",
        "    Explore the structure and content of the LanceDB table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        table = db.open_table(table_name)\n",
        "\n",
        "        print(\"Table Schema:\")\n",
        "        print(table.schema)\n",
        "\n",
        "        print(f\"\\nTotal records: {table.count_rows()}\")\n",
        "\n",
        "        print(\"\\nSample records:\")\n",
        "        df = table.to_pandas().head()\n",
        "        print(df)\n",
        "\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error exploring table: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_filtered_query_engine(db, table_name, embed_model, filter_dict=None):\n",
        "    \"\"\"\n",
        "    Create a query engine with metadata filtering capabilities\n",
        "    \"\"\"\n",
        "    from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "    # Reconnect to existing table\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=\"./lancedb_data\",\n",
        "        table_name=table_name,\n",
        "        mode=\"read\"\n",
        "    )\n",
        "\n",
        "    # Create index\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    # Create query engine with filters if provided\n",
        "    if filter_dict:\n",
        "        filters = MetadataFilters(\n",
        "            filters=[\n",
        "                MetadataFilter(\n",
        "                    key=key,\n",
        "                    value=value,\n",
        "                    operator=FilterOperator.EQ\n",
        "                ) for key, value in filter_dict.items()\n",
        "            ]\n",
        "        )\n",
        "        query_engine = index.as_query_engine(\n",
        "            filters=filters,\n",
        "            response_mode=\"tree_summarize\"\n",
        "        )\n",
        "    else:\n",
        "        query_engine = index.as_query_engine(\n",
        "            response_mode=\"tree_summarize\"\n",
        "        )\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "async def batch_process_documents(documents, batch_size=50):\n",
        "    \"\"\"\n",
        "    Process documents in batches for large datasets\n",
        "    \"\"\"\n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "        table_name = f\"personas_batch_{i//batch_size}\"\n",
        "\n",
        "        vector_store = LanceDBVectorStore(\n",
        "            uri=\"./lancedb_data\",\n",
        "            table_name=table_name,\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "\n",
        "        pipeline = IngestionPipeline(\n",
        "            transformations=[\n",
        "                SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
        "                embed_model,\n",
        "            ],\n",
        "            vector_store=vector_store,\n",
        "        )\n",
        "\n",
        "        nodes = await pipeline.arun(documents=batch)\n",
        "        print(f\"Processed batch {i//batch_size + 1}: {len(nodes)} nodes\")\n",
        "\n",
        "def show_usage_examples():\n",
        "    \"\"\"\n",
        "    Display usage examples for different scenarios\n",
        "    \"\"\"\n",
        "    print(\"Usage Examples:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    print(\"\\n1. Vector Search Only:\")\n",
        "    print(\"   test_vector_search()\")\n",
        "\n",
        "    print(\"\\n2. HuggingFace API RAG:\")\n",
        "    print(\"   # Set API token first\")\n",
        "    print(\"   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\")\n",
        "    print(\"   await test_huggingface_rag()\")\n",
        "\n",
        "    print(\"\\n3. Local LLM RAG:\")\n",
        "    print(\"   # Install and setup Ollama first\")\n",
        "    print(\"   setup_ollama()\")\n",
        "    print(\"   await test_local_llm_rag()\")\n",
        "\n",
        "    print(\"\\n4. Explore Database:\")\n",
        "    print(\"   explore_lancedb_table(db, table_name)\")\n",
        "\n",
        "# Show usage examples\n",
        "show_usage_examples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef63a577",
      "metadata": {
        "id": "ef63a577",
        "outputId": "1fa6db24-660e-4062-f34d-cc0516d08af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table Schema:\n",
            "id: string\n",
            "doc_id: string\n",
            "vector: fixed_size_list<item: float>[384]\n",
            "  child 0, item: float\n",
            "text: string\n",
            "metadata: struct<_node_content: string, _node_type: string, doc_id: string, document_id: string, persona_id: i (... 41 chars omitted)\n",
            "  child 0, _node_content: string\n",
            "  child 1, _node_type: string\n",
            "  child 2, doc_id: string\n",
            "  child 3, document_id: string\n",
            "  child 4, persona_id: int64\n",
            "  child 5, ref_doc_id: string\n",
            "  child 6, source: string\n",
            "\n",
            "Total records: 100\n",
            "\n",
            "Sample records:\n",
            "                                     id                                doc_id  \\\n",
            "0  60cb883b-dc1b-473e-bf1b-911526643e61  4ba97bc4-f109-4c80-8fe9-748c1194e69e   \n",
            "1  06694472-6c44-4656-9fde-675afd87fe26  105dbc32-cb12-4b26-a63a-dbfdedf37d81   \n",
            "2  10150826-af1c-4f3d-94d7-e1d16752bf89  873190dd-82aa-4f8a-9585-f8a4d2e35973   \n",
            "3  292bcb9f-7796-4fd3-b6eb-3d6edd36639f  2a590ad4-bcab-4a76-9dde-114d46c98160   \n",
            "4  67c222f2-3931-4951-8395-64f8b7dd39fc  152cb28f-bdee-4318-acde-9233165b32c2   \n",
            "\n",
            "                                              vector  \\\n",
            "0  [0.009862715, 0.036334712, 0.031320956, 0.0116...   \n",
            "1  [0.024929104, 0.07338048, 0.027212651, -0.0275...   \n",
            "2  [-0.045701314, 0.063290365, 0.0370313, -0.0345...   \n",
            "3  [-0.011168342, 0.066961035, 0.022813754, -0.02...   \n",
            "4  [-0.044002365, 0.029366117, 0.0010741628, -0.0...   \n",
            "\n",
            "                                                text  \\\n",
            "0  A local art historian and museum professional ...   \n",
            "1  An anthropologist or a cultural expert interes...   \n",
            "2  A military historian specializing in Japanese ...   \n",
            "3  An electrical engineering student or novice el...   \n",
            "4  A law professor or academic, likely with exper...   \n",
            "\n",
            "                                            metadata  \n",
            "0  {'_node_content': '{\"id_\": \"60cb883b-dc1b-473e...  \n",
            "1  {'_node_content': '{\"id_\": \"06694472-6c44-4656...  \n",
            "2  {'_node_content': '{\"id_\": \"10150826-af1c-4f3d...  \n",
            "3  {'_node_content': '{\"id_\": \"292bcb9f-7796-4fd3...  \n",
            "4  {'_node_content': '{\"id_\": \"67c222f2-3931-4951...  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LanceTable(name='personas_rag', version=1, _conn=LanceDBConnection(uri='/Users/divijbajaj/Library/CloudStorage/OneDrive-Personal/Personal_Projects/agents/lancedb_data'))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "explore_lancedb_table(db, table_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60273d9b",
      "metadata": {
        "id": "60273d9b"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides three complete RAG implementation approaches:\n",
        "\n",
        "### Option 1: Vector Search Only\n",
        "- **Best for**: Fast document retrieval, no generation needed\n",
        "- **Advantages**: Very fast, no API costs, no setup complexity\n",
        "- **Use case**: Finding relevant documents, initial exploration\n",
        "\n",
        "### Option 2: HuggingFace API\n",
        "- **Best for**: High-quality responses with cloud LLMs\n",
        "- **Advantages**: Latest models, no local resources needed\n",
        "- **Requirements**: HuggingFace API token\n",
        "- **Use case**: Production applications with budget for API calls\n",
        "\n",
        "### Option 3: Local LLM (Ollama)\n",
        "- **Best for**: Complete privacy, no internet dependency\n",
        "- **Advantages**: No API costs, full control, offline capability\n",
        "- **Requirements**: Ollama installation, local compute resources\n",
        "- **Use case**: Private data, cost-sensitive applications\n",
        "\n",
        "Choose the approach that best fits your needs!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}